{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192734\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "\n",
    "num_mol = 100000\n",
    "\n",
    "with open('ZINC.smiles', 'r') as file:\n",
    "    contents = file.readlines()\n",
    "    print(len(contents))\n",
    "    smi_total = []\n",
    "    logP_total= []\n",
    "    for i in range(num_mol):\n",
    "        smi = contents[i].strip()\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        smi_total.append(smi)\n",
    "        logP_total.append(MolLogP(mol))\n",
    "\n",
    "    logP_total= np.array(logP_total).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(logP_total, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "defaultdict(<class 'int'>, {1: 316826, 3: 443230, 2: 732489, 4: 16918})\n",
      "defaultdict(<class 'int'>, {2: 305225, 0: 589767, 1: 447459, 3: 167012})\n",
      "defaultdict(<class 'int'>, {2: 284794, 0: 653118, 1: 426879, 3: 144672})\n",
      "defaultdict(<class 'int'>, {'N': 205291, 'C': 1121069, 'O': 142317, 'F': 19232, 'Cl': 8161, 'S': 11903, 'Br': 1455, 'I': 35})\n"
     ]
    }
   ],
   "source": [
    "max_atom_n = 0\n",
    "atom_species = defaultdict(int)\n",
    "atom_degree = defaultdict(int)\n",
    "atom_H_n = defaultdict(int)\n",
    "atom_valence = defaultdict(int)\n",
    "\n",
    "for smi in smi_total:\n",
    "    mol = Chem.MolFromSmiles(smi.strip())\n",
    "    \n",
    "    adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
    "    max_atom_n = max(adj.shape[0], max_atom_n)\n",
    "    \n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_species[atom.GetSymbol()] += 1\n",
    "        atom_degree[atom.GetDegree()] += 1\n",
    "        atom_H_n[atom.GetTotalNumHs()] += 1\n",
    "        atom_valence[atom.GetImplicitValence()] += 1 \n",
    "\n",
    "    \n",
    "for inf in (max_atom_n, atom_degree, atom_H_n, atom_valence, atom_species):\n",
    "    print(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "def atom_feature(atom):\n",
    "    features =  list(map(lambda s: int(atom.GetDegree() == s), [1, 2, 3, 4])) + \\\n",
    "                list(map(lambda s: int(atom.GetTotalNumHs() == s), [0, 1, 2, 3])) + \\\n",
    "                list(map(lambda s: int(atom.GetImplicitValence() == s), [0, 1, 2, 3,])) + \\\n",
    "                list(map(lambda s: int(atom.GetSymbol() == s), ['C', 'N', 'O', 'S', 'F', 'Cl', 'Br','I',])) + \\\n",
    "                [atom.GetIsAromatic()]\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "num_features = len(atom_feature(atom))\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_to_feat_adj(smiles_list, num_features):\n",
    "    all_adj = []\n",
    "    all_features = []\n",
    "    all_index = []\n",
    "    \n",
    "    for smi in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smi.strip())\n",
    "        adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
    "        adj = sp.csr_matrix(np.array(adj, dtype='int64')).tocsr()\n",
    "\n",
    "        feature = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            feature.append( atom_feature(atom) ) \n",
    "\n",
    "        all_features.append(np.array(feature))\n",
    "        all_adj.append(adj)\n",
    "        all_index.append(len(feature))\n",
    "    \n",
    "    all_index = np.repeat(np.arange(len(all_index)), all_index)\n",
    "\n",
    "    #to tensor\n",
    "    all_adj = sp.block_diag(all_adj)\n",
    "    all_adj = tf.SparseTensor(\n",
    "        indices=np.array([all_adj.row, all_adj.col]).T,\n",
    "        values=all_adj.data,\n",
    "        dense_shape=all_adj.shape    \n",
    "    )\n",
    "    \n",
    "    all_features = np.vstack(all_features)\n",
    "    all_features = tf.convert_to_tensor(all_features)\n",
    "    \n",
    "    all_index  = tf.convert_to_tensor(all_index)\n",
    "    return all_features, all_adj, all_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 80000\n",
    "num_validation = 10000\n",
    "num_test = 10000\n",
    "\n",
    "smi_train = smi_total[0:num_train]\n",
    "logP_train = logP_total[0:num_train]\n",
    "smi_validation = smi_total[num_train:(num_train+num_validation)]\n",
    "logP_validation = logP_total[num_train:(num_train+num_validation)]\n",
    "smi_test = smi_total[(num_train+num_validation):]\n",
    "logP_test = logP_total[(num_train+num_validation):]\n",
    "\n",
    "def data_batchs(smi_list, logP, batch_size):\n",
    "    logP = tf.convert_to_tensor(logP)\n",
    "    i = 0\n",
    "    X, A, I, y = [], [], [], []\n",
    "    \n",
    "    while i < len(smi_list):\n",
    "\n",
    "        batch_features, batch_adj, batch_index = smi_to_feat_adj(smi_list[i:i+batch_size], num_features)\n",
    "        X.append(batch_features)\n",
    "        A.append(batch_adj)\n",
    "        I.append(batch_index)\n",
    "        y.append(logP[i:i+batch_size])\n",
    "\n",
    "        i += batch_size\n",
    "    \n",
    "    return X, A, I, y\n",
    "\n",
    "X_train, A_train, I_train, logP_train = data_batchs(smi_train, logP_train, 25)\n",
    "X_validation, A_validation, I_validation, logP_validation = data_batchs(smi_validation, logP_validation, 500)\n",
    "X_test, A_test, I_test, logP_test = data_batchs(smi_test, logP_test, 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Layer, Dropout\n",
    "from tensorflow.keras import activations, initializers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "class Readout(Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = Dense(units=self.units, \n",
    "                             activation=None, \n",
    "                             use_bias=True, \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros',)    \n",
    "\n",
    "    def call(self, input_X, input_I):\n",
    "        output_Z = self.dense(input_X)\n",
    "        output_Z = tf.math.segment_sum(output_Z, input_I)\n",
    "        output_Z = tf.nn.sigmoid(output_Z)\n",
    "        return output_Z    \n",
    "\n",
    "\n",
    "class GatedGCN(Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.inp_dim = int(input_shape[-1])\n",
    "        if(self.units != self.inp_dim):\n",
    "            self.dense_i = Dense(\n",
    "                units=self.units, \n",
    "                activation=None, \n",
    "                use_bias=True, \n",
    "                kernel_initializer='glorot_uniform', \n",
    "                bias_initializer='zeros',)\n",
    "\n",
    "        self.dense_n = Dense(units=self.units, \n",
    "                             activation=None, \n",
    "                             use_bias=True, \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros',)    \n",
    "        \n",
    "        self.dense_gate_n = Dense(units=self.units, \n",
    "                             activation=None, \n",
    "                             use_bias=True, \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros',)\n",
    "        \n",
    "        self.dense_gate_i = Dense(units=self.units, \n",
    "                             activation=None, \n",
    "                             use_bias=True, \n",
    "                             kernel_initializer='glorot_uniform', \n",
    "                             bias_initializer='zeros',)\n",
    "\n",
    "    def call(self, input_X, input_A):\n",
    "        new_X = self.dense_n(input_X)\n",
    "        new_X = tf.sparse.sparse_dense_matmul(input_A, new_X)\n",
    "        \n",
    "        X1 = self.dense_gate_i(input_X)\n",
    "        X2 = self.dense_gate_n(new_X)\n",
    "        gate_coefficient = tf.nn.sigmoid(X1 + X2)\n",
    "\n",
    "        if(self.units != self.inp_dim):\n",
    "            input_X = self.dense_i(input_X)\n",
    "            \n",
    "        output_X = tf.multiply(new_X, gate_coefficient) + tf.multiply(input_X, 1.0-gate_coefficient)        \n",
    "        \n",
    "        return output_X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 21)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gated_gcn (GatedGCN)            (None, 64)           8384        input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gated_gcn_1 (GatedGCN)          (None, 64)           12480       gated_gcn[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gated_gcn_2 (GatedGCN)          (None, 64)           12480       gated_gcn_1[0][0]                \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gated_gcn_3 (GatedGCN)          (None, 64)           12480       gated_gcn_2[0][0]                \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "readout (Readout)               (None, 256)          16640       gated_gcn_3[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          65792       readout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 194,305\n",
      "Trainable params: 194,305\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features=21\n",
    "\n",
    "num_layer = 4\n",
    "hidden_dim1 = 64\n",
    "hidden_dim2 = 256\n",
    "init_lr = 0.0001\n",
    "\n",
    "X = Input(shape=(num_features,))\n",
    "A = Input(shape=(None,), sparse=True)\n",
    "I = Input(shape=(), dtype=tf.int64)\n",
    "\n",
    "h = X\n",
    "\n",
    "for i in range(num_layer):\n",
    "    h = GatedGCN(units=hidden_dim1)(h, A)\n",
    "\n",
    "h = Readout(units=hidden_dim2)(h, I) \n",
    "\n",
    "h = Dense(units=hidden_dim2, use_bias=True, activation='relu')(h)\n",
    "h = Dense(units=hidden_dim2, use_bias=True, activation='tanh')(h)\n",
    "h = Dropout(0.5)(h)\n",
    "Y_pred = Dense(units=1, use_bias=True)(h)\n",
    "\n",
    "model = Model(inputs=[X, A, I], outputs=Y_pred)\n",
    "optimizer = Adam(lr=init_lr)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def train_step(x, a, i, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([x, a, i], training=True)\n",
    "        loss = MSE(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def test_step(x, a, i, y):\n",
    "    predictions = model([x, a, i], training=False)\n",
    "    loss = MSE(y, predictions)\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0   Loss:0.364   val_Loss:0.192\n",
      "Epoch:1   Loss:0.126   val_Loss:0.102\n",
      "Epoch:2   Loss:0.089   val_Loss:0.067\n",
      "Epoch:3   Loss:0.072   val_Loss:0.070\n",
      "Epoch:4   Loss:0.061   val_Loss:0.050\n",
      "Epoch:5   Loss:0.052   val_Loss:0.040\n",
      "Epoch:6   Loss:0.048   val_Loss:0.046\n",
      "Epoch:7   Loss:0.043   val_Loss:0.034\n",
      "Epoch:8   Loss:0.039   val_Loss:0.037\n",
      "Epoch:9   Loss:0.037   val_Loss:0.028\n",
      "Epoch:10   Loss:0.035   val_Loss:0.029\n",
      "Epoch:11   Loss:0.032   val_Loss:0.025\n",
      "Epoch:12   Loss:0.030   val_Loss:0.020\n",
      "Epoch:13   Loss:0.030   val_Loss:0.021\n",
      "Epoch:14   Loss:0.027   val_Loss:0.022\n",
      "Epoch:15   Loss:0.028   val_Loss:0.021\n",
      "Epoch:16   Loss:0.025   val_Loss:0.023\n",
      "Epoch:17   Loss:0.024   val_Loss:0.023\n",
      "Epoch:18   Loss:0.023   val_Loss:0.014\n",
      "Epoch:19   Loss:0.023   val_Loss:0.020\n",
      "Epoch:20   Loss:0.022   val_Loss:0.016\n",
      "Epoch:21   Loss:0.022   val_Loss:0.018\n",
      "Epoch:22   Loss:0.021   val_Loss:0.016\n",
      "Epoch:23   Loss:0.020   val_Loss:0.013\n",
      "Epoch:24   Loss:0.020   val_Loss:0.014\n",
      "Epoch:25   Loss:0.020   val_Loss:0.024\n",
      "Epoch:26   Loss:0.020   val_Loss:0.013\n",
      "Epoch:27   Loss:0.018   val_Loss:0.012\n",
      "Epoch:28   Loss:0.018   val_Loss:0.011\n",
      "Epoch:29   Loss:0.018   val_Loss:0.012\n",
      "Epoch:30   Loss:0.017   val_Loss:0.010\n",
      "Epoch:31   Loss:0.017   val_Loss:0.019\n",
      "Epoch:32   Loss:0.017   val_Loss:0.011\n",
      "Epoch:33   Loss:0.016   val_Loss:0.011\n",
      "Epoch:34   Loss:0.016   val_Loss:0.010\n",
      "Epoch:35   Loss:0.016   val_Loss:0.010\n",
      "Epoch:36   Loss:0.016   val_Loss:0.013\n",
      "Epoch:37   Loss:0.015   val_Loss:0.010\n",
      "Epoch:38   Loss:0.015   val_Loss:0.014\n",
      "Epoch:39   Loss:0.015   val_Loss:0.008\n",
      "Epoch:40   Loss:0.015   val_Loss:0.010\n",
      "Epoch:41   Loss:0.014   val_Loss:0.014\n",
      "Epoch:42   Loss:0.014   val_Loss:0.008\n",
      "Epoch:43   Loss:0.014   val_Loss:0.011\n",
      "Epoch:44   Loss:0.014   val_Loss:0.010\n",
      "Epoch:45   Loss:0.014   val_Loss:0.012\n",
      "Epoch:46   Loss:0.014   val_Loss:0.010\n",
      "Epoch:47   Loss:0.013   val_Loss:0.010\n",
      "Epoch:48   Loss:0.013   val_Loss:0.009\n",
      "Epoch:49   Loss:0.013   val_Loss:0.008\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 50\n",
    "\n",
    "try:\n",
    "    for epoch_n in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_n = 0\n",
    "        for x, a, i, y in zip(X_train, A_train, I_train, logP_train):\n",
    "            loss = train_step(x, a, i, y)\n",
    "            epoch_loss += loss\n",
    "            batch_n += 1\n",
    "\n",
    "            if batch_n == len(X_train):\n",
    "\n",
    "                val_loss = 0\n",
    "                val_n = 0\n",
    "                for x, a, i, y in zip(X_validation, A_validation, I_validation, logP_validation):\n",
    "                    val_loss += test_step(x, a, i, y)\n",
    "                    val_n += 1\n",
    "                print(f'Epoch:{epoch_n}   Loss:{epoch_loss/len(X_train):.3f}   val_Loss:{val_loss/val_n:.3f}')\n",
    "                epoch_loss = 0\n",
    "                batch_n = 0\n",
    "\n",
    "                # shuffle\n",
    "                c = list(zip(X_train, A_train, I_train, logP_train))\n",
    "                random.shuffle(c)\n",
    "                X_train, A_train, I_train, logP_train = zip(*c)\n",
    "\n",
    "except:\n",
    "    print('Early stoping')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Loss:0.009\n"
     ]
    }
   ],
   "source": [
    "test_n, test_loss = 0, 0\n",
    "for x, a, i, y in zip(X_test, A_test, I_test, logP_test):\n",
    "    test_loss += test_step(x, a, i, y)\n",
    "    test_n += 1\n",
    "print(f'test_Loss:{test_loss/test_n:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.3.1\n",
      "scipy==1.3.1\n",
      "numpy==1.19.1\n",
      "matplotlib==3.1.1\n",
      "builtins\n",
      "builtins\n",
      "numpy\n",
      "tensorflow\n",
      "rdkit.Chem\n",
      "rdkit.DataStructs\n",
      "rdkit.Chem.AllChem\n",
      "matplotlib.pyplot\n",
      "time\n",
      "datetime\n",
      "random\n",
      "scipy.sparse\n",
      "tensorflow.keras.activations\n",
      "tensorflow.keras.initializers\n",
      "pkg_resources\n",
      "types\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))\n",
    "    \n",
    "for name, val in globals().items():\n",
    "    if isinstance(val, types.ModuleType):\n",
    "        print(val.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
